{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cirrus Demo\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "This will run a simple logistic regression on the Criteo Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 (default, Dec  4 2017, 14:50:18) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.genUID = function() {\n",
       "    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {\n",
       "        var r = Math.random()*16|0, v = c == 'x' ? r : (r&0x3|0x8);\n",
       "        return v.toString(16);\n",
       "    });\n",
       "};\n",
       "\n",
       "\n",
       "define('graphWidget', [\"@jupyter-widgets/base\"], function (widget) {\n",
       "\n",
       "    var GraphView = widget.DOMWidgetView.extend({\n",
       "        render: function(){\n",
       "            var that = this;\n",
       "\n",
       "            var graphId = window.genUID();\n",
       "            var loadingId = 'loading-'+graphId;\n",
       "\n",
       "\n",
       "            var _graph_url = that.model.get('_graph_url');\n",
       "\n",
       "            // variable plotlyDomain in the case of enterprise\n",
       "            var url_parts = _graph_url.split('/');\n",
       "            var plotlyDomain = url_parts[0] + '//' + url_parts[2];\n",
       "\n",
       "            if(!('plotlyDomains' in window)){\n",
       "                window.plotlyDomains = {};\n",
       "            }\n",
       "            window.plotlyDomains[graphId] = plotlyDomain;\n",
       "\n",
       "            // Place IFrame in output cell div `$el`\n",
       "            that.$el.css('width', '100%');\n",
       "            that.$graph = $(['<iframe id=\"'+graphId+'\"',\n",
       "                             'src=\"'+_graph_url+'.embed\"',\n",
       "                             'seamless',\n",
       "                             'style=\"border: none;\"',\n",
       "                             'width=\"100%\"',\n",
       "                             'height=\"600\">',\n",
       "                             '</iframe>'].join(' '));\n",
       "            that.$graph.appendTo(that.$el);\n",
       "\n",
       "            that.$loading = $('<div id=\"'+loadingId+'\">Initializing...</div>')\n",
       "                            .appendTo(that.$el);\n",
       "\n",
       "            // for some reason the 'width' is being changed in IPython 3.0.0\n",
       "            // for the containing `div` element. There's a flicker here, but\n",
       "            // I was unable to fix it otherwise.\n",
       "            setTimeout(function ()  {\n",
       "                if (IPYTHON_VERSION === '3') {\n",
       "                    $('#' + graphId)[0].parentElement.style.width = '100%';\n",
       "                }\n",
       "            }, 500);\n",
       "\n",
       "            // initialize communication with the iframe\n",
       "            if(!('pingers' in window)){\n",
       "                window.pingers = {};\n",
       "            }\n",
       "\n",
       "            window.pingers[graphId] = setInterval(function() {\n",
       "                that.graphContentWindow = $('#'+graphId)[0].contentWindow;\n",
       "                that.graphContentWindow.postMessage({task: 'ping'}, plotlyDomain);\n",
       "            }, 200);\n",
       "\n",
       "            // Assign a message listener to the 'message' events\n",
       "            // from iframe's postMessage protocol.\n",
       "            // Filter the messages by iframe src so that the right message\n",
       "            // gets passed to the right widget\n",
       "            if(!('messageListeners' in window)){\n",
       "                 window.messageListeners = {};\n",
       "            }\n",
       "\n",
       "            window.messageListeners[graphId] = function(e) {\n",
       "                if(_graph_url.indexOf(e.origin)>-1) {\n",
       "                    var frame = document.getElementById(graphId);\n",
       "\n",
       "                    if(frame === null){\n",
       "                        // frame doesn't exist in the dom anymore, clean up it's old event listener\n",
       "                        window.removeEventListener('message', window.messageListeners[graphId]);\n",
       "                        clearInterval(window.pingers[graphId]);\n",
       "                    } else if(frame.contentWindow === e.source) {\n",
       "                        // TODO: Stop event propagation, so each frame doesn't listen and filter\n",
       "                        var frameContentWindow = $('#'+graphId)[0].contentWindow;\n",
       "                        var message = e.data;\n",
       "\n",
       "                        if('pong' in message && message.pong) {\n",
       "                            $('#loading-'+graphId).hide();\n",
       "                            clearInterval(window.pingers[graphId]);\n",
       "                            that.send({event: 'pong', graphId: graphId});\n",
       "                        } else if (message.type==='hover' ||\n",
       "                                   message.type==='zoom'  ||\n",
       "                                   message.type==='click' ||\n",
       "                                   message.type==='unhover') {\n",
       "\n",
       "                            // click and hover events contain all of the data in the traces,\n",
       "                            // which can be a very large object and may take a ton of time\n",
       "                            // to pass to the python backend. Strip out the data, and require\n",
       "                            // the user to call get_figure if they need trace information\n",
       "                            if(message.type !== 'zoom') {\n",
       "                                for(var i in message.points) {\n",
       "                                    delete message.points[i].data;\n",
       "                                    delete message.points[i].fullData;\n",
       "                                }\n",
       "                            }\n",
       "                            that.send({event: message.type, message: message, graphId: graphId});\n",
       "                        } else if (message.task === 'getAttributes') {\n",
       "                            that.send({event: 'getAttributes', response: message.response});\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            };\n",
       "\n",
       "            window.removeEventListener('message', window.messageListeners[graphId]);\n",
       "            window.addEventListener('message', window.messageListeners[graphId]);\n",
       "\n",
       "        },\n",
       "\n",
       "        update: function() {\n",
       "            // Listen for messages from the graph widget in python\n",
       "            var jmessage = this.model.get('_message');\n",
       "            var message = JSON.parse(jmessage);\n",
       "\n",
       "            // check for duplicate messages\n",
       "            if(!('messageIds' in window)){\n",
       "                window.messageIds = {};\n",
       "            }\n",
       "\n",
       "            if(!(message.uid in window.messageIds)){\n",
       "                // message hasn't been received yet, do stuff\n",
       "                window.messageIds[message.uid] = true;\n",
       "\n",
       "                if (message.fadeTo) {\n",
       "                    this.fadeTo(message);\n",
       "                } else {\n",
       "                    var plot = $('#' + message.graphId)[0].contentWindow;\n",
       "                    plot.postMessage(message, window.plotlyDomains[message.graphId]);\n",
       "                }\n",
       "            }\n",
       "\n",
       "            return GraphView.__super__.update.apply(this);\n",
       "        },\n",
       "\n",
       "        /**\n",
       "         * Wrapper for jquery's `fadeTo` function.\n",
       "         *\n",
       "         * @param message Contains the id we need to find the element.\n",
       "         */\n",
       "        fadeTo: function (message) {\n",
       "            var plot = $('#' + message.graphId);\n",
       "            plot.fadeTo(message.duration, message.opacity);\n",
       "        }\n",
       "    });\n",
       "\n",
       "    // Register the GraphView with the widget manager.\n",
       "    return {\n",
       "        GraphView: GraphView\n",
       "    }\n",
       "\n",
       "});\n",
       "\n",
       "//@ sourceURL=graphWidget.js\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cirrus\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import tools\n",
    "import plotly.tools as tls   \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "from __future__ import print_function\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display, clear_output\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.widgets import GraphWidget\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import random\n",
    "\n",
    "class mock():\n",
    "    \n",
    "    def __init__(self, strid):\n",
    "        self.pipe = py.Stream(strid)\n",
    "        self.pipe.open()\n",
    "        self.kill_sig = threading.Event()\n",
    "    \n",
    "    def start_thread(self):\n",
    "        \n",
    "        def num_producer():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            while not self.kill_sig.is_set():\n",
    "                time.sleep(0.5)\n",
    "                now_time = time.time()\n",
    "                integer = random.random()\n",
    "                self.pipe.write(dict(x = now_time - start_time, y = integer))\n",
    "        \n",
    "        self.thr = threading.Thread(target=num_producer)\n",
    "        self.thr.start()\n",
    "        \n",
    "    def kill(self):\n",
    "        print(\"Mock received kill command\")\n",
    "        self.kill_sig.set()\n",
    "        self.thr.join()\n",
    "        self.pipe.close()\n",
    "        print(\"Mock is dead\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression workload\n",
      "Starting CollaborativeFilteringTask\n",
      "Running Logistic Regression workload\n",
      "Starting CollaborativeFilteringTask\n"
     ]
    }
   ],
   "source": [
    "import collab\n",
    "\n",
    "\n",
    "\n",
    "data_bucket = 'cirrus-criteo-kaggle-19b-random'\n",
    "model = 'model_v1'\n",
    "\n",
    "lr_task = collab.CollaborativeFiltering(\n",
    "             # number of workers\n",
    "             n_workers = 5,\n",
    "             # number of parameter servers\n",
    "             n_ps = 2,\n",
    "             # worker size in MB\n",
    "             worker_size = 128,\n",
    "             # path to s3 bucket with input dataset\n",
    "             dataset = data_bucket,\n",
    "             # sgd update LR and epsilon\n",
    "             learning_rate=0.01,\n",
    "             epsilon=0.0001,\n",
    "             progress_callback = print,\n",
    "             # stop workload after these many seconds\n",
    "             timeout = 0,\n",
    "             # stop workload once we reach this loss\n",
    "             threshold_loss=0,\n",
    "             # resume execution from model stored in this s3 bucket\n",
    "             resume_model = model,\n",
    "             # aws key name\n",
    "             key_name='mykey',\n",
    "             # path to aws key\n",
    "             key_path='/home/camus/Downloads/mykey.pem',\n",
    "             # ip where ps lives\n",
    "             ps_ip_public='ec2-54-188-0-164.us-west-2.compute.amazonaws.com',\n",
    "             ps_ip_private='172.31.26.54',\n",
    "             # username of VM\n",
    "             ps_username='ubuntu',\n",
    "             # choose between adagrad, sgd, nesterov, momentum\n",
    "             opt_method = 'adagrad',\n",
    "             # checkpoint model every x secs\n",
    "             checkpoint_model = 60,\n",
    "             #\n",
    "             minibatch_size=20,\n",
    "             # model size\n",
    "             model_bits=19,\n",
    "             # whether to filter gradient weights\n",
    "             use_grad_threshold=False,\n",
    "             # threshold value\n",
    "             grad_threshold=0.001,\n",
    "             # range of training minibatches\n",
    "             train_set=(0,824),\n",
    "             # range of testing minibatches\n",
    "             test_set=(835,840)\n",
    "             )\n",
    "\n",
    "lr_task1 = collab.CollaborativeFiltering(\n",
    "             # number of workers\n",
    "             n_workers = 5,\n",
    "             # number of parameter servers\n",
    "             n_ps = 2,\n",
    "             # worker size in MB\n",
    "             worker_size = 128,\n",
    "             # path to s3 bucket with input dataset\n",
    "             dataset = data_bucket,\n",
    "             # sgd update LR and epsilon\n",
    "             learning_rate=0.01,\n",
    "             epsilon=0.0001,\n",
    "             progress_callback = print,\n",
    "             # stop workload after these many seconds\n",
    "             timeout = 0,\n",
    "             # stop workload once we reach this loss\n",
    "             threshold_loss=0,\n",
    "             # resume execution from model stored in this s3 bucket\n",
    "             resume_model = model,\n",
    "             # aws key name\n",
    "             key_name='mykey',\n",
    "             # path to aws key\n",
    "             key_path='/home/camus/Downloads/mykey.pem',\n",
    "             # ip where ps lives\n",
    "             ps_ip_public='ec2-54-188-0-164.us-west-2.compute.amazonaws.com',\n",
    "             ps_ip_private='172.31.26.54',\n",
    "             # username of VM\n",
    "             ps_username='ubuntu',\n",
    "             # choose between adagrad, sgd, nesterov, momentum\n",
    "             opt_method = 'adagrad',\n",
    "             # checkpoint model every x secs\n",
    "             checkpoint_model = 60,\n",
    "             #\n",
    "             minibatch_size=20,\n",
    "             # model size\n",
    "             model_bits=19,\n",
    "             # whether to filter gradient weights\n",
    "             use_grad_threshold=False,\n",
    "             # threshold value\n",
    "             grad_threshold=0.001,\n",
    "             # range of training minibatches\n",
    "             train_set=(0,824),\n",
    "             # range of testing minibatches\n",
    "             test_set=(835,840)\n",
    "             )\n",
    "\n",
    "\n",
    "#model, loss = lr_task.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://plot.ly/~andrewmzhang/16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc08be5570a409c96c1c73b256b8c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'y': 1.02934, 'x': 14.478, 'pointNumber': 13, 'curveNumber': 1}]\n",
      "PS has 7 lambdas\n",
      "Current training loss: (16.566, 1.02569) current cost ($):  0.0015346577563\n",
      "Current training loss: (17.588, 1.02479) current cost ($):  0.00153481244354\n",
      "Error task has received kill signal\n",
      "Lambda launcher has received kill signal\n",
      "Everyone is dead\n",
      "Current training loss: (18.609, 1.02353) current cost ($):  0.00166283230705\n",
      "Current training loss: (19.641, 1.02172) current cost ($):  0.00166313542112\n",
      "PS has 7 lambdas\n",
      "Current training loss: (20.672, 1.02049) current cost ($):  0.00179103554637\n",
      "Current training loss: (21.71, 1.01999) current cost ($):  0.0017912914945\n",
      "PS has 7 lambdas\n",
      "Current training loss: (22.753, 1.01849) current cost ($):  0.00191930894197\n",
      "PS has 7 lambdas\n",
      "Current training loss: (23.803, 1.01777) current cost ($):  0.00204710122681\n",
      "Current training loss: (24.834, 1.01625) current cost ($):  0.00204730880858\n",
      "Current training loss: (25.861, 1.01558) current cost ($):  0.00217509528885\n",
      "Current training loss: (26.894, 1.01441) current cost ($):  0.00217521783822\n",
      "PS has 7 lambdas\n",
      "Current training loss: (27.95, 1.01333) current cost ($):  0.00230285251834\n",
      "Current training loss: (28.97, 1.01209) current cost ($):  0.00230290764656\n",
      "PS has 7 lambdas\n",
      "Current training loss: (30.014, 1.01171) current cost ($):  0.00243053504817\n",
      "Current training loss: (31.043, 1.01135) current cost ($):  0.00243068991776\n",
      "PS has 7 lambdas\n",
      "Current training loss: (32.068, 1.0097) current cost ($):  0.00255874407686\n",
      "Current training loss: (33.103, 1.00871) current cost ($):  0.00255893495909\n",
      "Current training loss: (34.152, 1.00776) current cost ($):  0.00268661403961\n",
      "Current training loss: (35.186, 1.00724) current cost ($):  0.00268676439622PS has 7 lambdas\n",
      "\n",
      "Current training loss: (36.215, 1.00662) current cost ($):  0.00281495889829\n",
      "Current training loss: (37.25, 1.00658) current cost ($):  0.00281511199004\n",
      "PS has 7 lambdas\n",
      "Current training loss: (38.306, 1.00535) current cost ($):  0.00294313147367\n",
      "Current training loss: (39.349, 1.00427) current cost ($):  0.00294333598601\n",
      "PS has 7 lambdas\n",
      "Current training loss: (40.39, 1.00296) current cost ($):  0.00307116147238\n",
      "Current training loss: (41.429, 1.00269) current cost ($):  0.0030714850544\n",
      "PS has 7 lambdas\n",
      "Current training loss: (42.463, 1.0023) current cost ($):  0.00319924585476\n",
      "Current training loss: (43.515, 1.00177) current cost ($):  0.00319947668521\n",
      "Current training loss: (44.557, 1.00149) current cost ($):  0.00332711645571\n",
      "Current training loss: (45.605, 1.00145) current cost ($):  0.00332746226832\n",
      "PS has 7 lambdas\n",
      "Current training loss: (46.647, 1.0017) current cost ($):  0.00345537552223\n",
      "Current training loss: (47.677, 1.0016) current cost ($):  0.00345577047615\n",
      "Current training loss: (48.709, 1.00231) current cost ($):  0.00358382890511\n",
      "Current training loss: (49.736, 1.00312) current cost ($):  0.00358387096545\n",
      "PS has 7 lambdas\n",
      "Current training loss: (50.761, 1.0032) current cost ($):  0.00371140411116\n",
      "Current training loss: (51.786, 1.00349) current cost ($):  0.0037116089426\n",
      "PS has 7 lambdas\n",
      "Current training loss: (52.807, 1.00283) current cost ($):  0.00383928199062\n",
      "PS has 7 lambdas\n",
      "Current training loss: (53.824, 1.00224) current cost ($):  0.00396702780851\n",
      "Current training loss: (54.867, 1.00148) current cost ($):  0.00396706534068\n",
      "Current training loss: (55.92, 1.00085) current cost ($):  0.00409468930817\n",
      "Current training loss: (56.941, 1.00017) current cost ($):  0.00409475259622\n",
      "PS has 7 lambdas\n",
      "Current training loss: (57.96, 0.99936) current cost ($):  0.00422240121034\n",
      "Current training loss: (58.996, 0.999464) current cost ($):  0.00422248139547\n",
      "PS has 7 lambdas\n",
      "Current training loss: (60.02, 0.999648) current cost ($):  0.00435020662384\n",
      "Current training loss: (61.066, 1.00079) current cost ($):  0.00435056468379\n",
      "PS has 7 lambdas\n",
      "Current training loss: (62.087, 1.00141) current cost ($):  0.00447831820564\n",
      "Current training loss: (63.15, 1.00207) current cost ($):  0.00447850794824\n",
      "Current training loss: (64.213, 1.00246) current cost ($):  0.00460632036673\n",
      "Current training loss: (65.233, 1.00266) current cost ($):  0.00460639269594\n",
      "Current training loss: (66.27, 1.00254) current cost ($):  0.00473393470046\n",
      "Current training loss: (67.304, 1.00296) current cost ($):  0.00473414559479\n",
      "Current training loss: (68.339, 1.00362) current cost ($):  0.00486173277467\n",
      "Current training loss: (69.363, 1.00346) current cost ($):  0.00486179809888\n",
      "Current training loss: (70.4, 1.00306) current cost ($):  0.00498945049661\n",
      "Current training loss: (71.42, 1.00281) current cost ($):  0.00498970185579\n",
      "Current training loss: (72.44, 1.00251) current cost ($):  0.0051175110225\n",
      "Current training loss: (73.479, 1.00194) current cost ($):  0.00511779031048\n",
      "Current training loss: (74.52, 1.00177) current cost ($):  0.00524545454528\n",
      "Current training loss: (75.54, 1.0018) current cost ($):  0.0052458596344\n",
      "Current training loss: (76.607, 1.00137) current cost ($):  0.00537357645982\n",
      "Current training loss: (77.626, 1.0009) current cost ($):  0.00537364477749\n",
      "Current training loss: (78.67, 1.00085) current cost ($):  0.00550118907649\n",
      "Current training loss: (79.723, 1.00106) current cost ($):  0.00550151818956\n",
      "PS has 7 lambdas\n",
      "Current training loss: (80.736, 1.00131) current cost ($):  0.00562934068247\n",
      "Current training loss: (81.749, 1.00172) current cost ($):  0.00562950383345\n",
      "Current training loss: (82.782, 1.00153) current cost ($):  0.00575739579887\n",
      "Current training loss: (83.827, 1.00158) current cost ($):  0.00575747119751\n",
      "Current training loss: (84.838, 1.00189) current cost ($):  0.00588512108803\n",
      "Current training loss: (85.888, 1.00223) current cost ($):  0.0058853864116\n",
      "Current training loss: (86.899, 1.00281) current cost ($):  0.00601309551786\n",
      "Current training loss: (87.944, 1.00378) current cost ($):  0.0060132676644\n",
      "Current training loss: (88.989, 1.00348) current cost ($):  0.00614092439276\n",
      "PS has 7 lambdas\n",
      "Current training loss: (90.01, 1.00322) current cost ($):  0.00626854273802\n",
      "Current training loss: (91.06, 1.00266) current cost ($):  0.00626886797632\n",
      "Current training loss: (92.08, 1.00168) current cost ($):  0.00639685877139\n",
      "Current training loss: (93.151, 1.0012) current cost ($):  0.00639707267437\n",
      "PS has 7 lambdas\n",
      "Current training loss: (94.22, 0.999776) current cost ($):  0.00652512962939\n",
      "Current training loss: (95.263, 0.998851) current cost ($):  0.00652519285666\n",
      "Current training loss: (96.277, 0.998312) current cost ($):  0.00665277181594\n",
      "Current training loss: (97.3, 0.997696) current cost ($):  0.00665321782576\n",
      "Current training loss: (98.34, 0.997296) current cost ($):  0.00678095089486\n",
      "Current training loss: (99.38, 0.99773) current cost ($):  0.00678107657445\n",
      "Current training loss: (100.39, 0.998104) current cost ($):  0.00690870073948\n",
      "Current training loss: (101.4, 0.998465) current cost ($):  0.00690896911729\n",
      "Current training loss: (102.45, 0.995841) current cost ($):  0.00703687146104\n",
      "Current training loss: (103.4, 0.995685) current cost ($):  0.00703693417168\n",
      "Current training loss: (104.53, 0.997107) current cost ($):  0.00716451282705\n",
      "Current training loss: (105.55, 0.997222) current cost ($):  0.00716484513111\n",
      "Current training loss: (106.58, 0.997465) current cost ($):  0.00729262478873\n",
      "Current training loss: (107.63, 0.998441) current cost ($):  0.00729298686021\n"
     ]
    }
   ],
   "source": [
    "stream_ids = tls.get_credentials_file()['stream_ids']\n",
    "\n",
    "# Get stream id from stream id list \n",
    "stream_id0 = stream_ids[0]\n",
    "stream_id1 = stream_ids[1]\n",
    "stream_id2 = stream_ids[2]\n",
    "stream_id3 = stream_ids[3]\n",
    "\n",
    "\n",
    "stream_0 = go.Stream(\n",
    "    token=stream_id0,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_1 = go.Stream(\n",
    "    token=stream_id1,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_2 = go.Stream(\n",
    "    token=stream_id2,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_3 = go.Stream(\n",
    "    token=stream_id3,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    mode='lines+markers',\n",
    "    stream=stream_0         # (!) embed stream id, 1 per trace\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    stream=stream_1\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    stream=stream_2\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    stream=stream_3\n",
    ")\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        domain=[0, 0.45]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        domain=[0, 0.45]\n",
    "    ),\n",
    "    xaxis2=dict(\n",
    "        domain=[0.55, 1]\n",
    "    ),\n",
    "    xaxis3=dict(\n",
    "        domain=[0, 0.45],\n",
    "        anchor='y3'\n",
    "    ),\n",
    "    xaxis4=dict(\n",
    "        domain=[0.55, 1],\n",
    "        anchor='y4'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        domain=[0, 0.45],\n",
    "        anchor='x2'\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        domain=[0.55, 1]\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        domain=[0.55, 1],\n",
    "        anchor='x4'\n",
    "    )\n",
    ")\n",
    "\n",
    "    \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "url = py.plot(fig, filename='multiple-subplots', auto_open=False)\n",
    "print(url)\n",
    "g = GraphWidget(url)\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_handler(widget, msg):\n",
    "    print(msg)\n",
    "    global lr_task\n",
    "    if msg[0]['curveNumber'] == 0:\n",
    "        lr_task.kill()\n",
    "    if msg[0]['curveNumber'] == 1:\n",
    "        lr_task1.kill()\n",
    "\n",
    "pipe0 = py.Stream(stream_id0)\n",
    "pipe0.open()\n",
    "\n",
    "pipe1 = py.Stream(stream_id1)\n",
    "pipe1.open()\n",
    "\n",
    "def progress_callback0(time_loss, cost, task):\n",
    "    global pipe0\n",
    "    print(\"Current training loss:\", time_loss, \"current cost ($): \", cost)\n",
    "    pipe0.write(dict(x=time_loss[0], y=time_loss[1]))\n",
    "    \n",
    "def progress_callback1(time_loss, cost, task):\n",
    "    global pipe1\n",
    "    print(\"Current training loss:\", time_loss, \"current cost ($): \", cost)\n",
    "    pipe1.write(dict(x=time_loss[0], y=time_loss[1]))\n",
    "\n",
    "\n",
    "lr_task.progress_callback = progress_callback0\n",
    "lr_task1.progress_callback = progress_callback1\n",
    "\n",
    "\n",
    "g.on_click(message_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d13fde9c9646cd9ccb0c1198aca12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description=u'Halt!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(description=\"Halt!\")\n",
    "display(button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    lr_task.kill()\n",
    "    lr_task1.kill()\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's specific ip: ec2-54-188-0-164.us-west-2.compute.amazonaws.com\n",
      "Copying ps to vm\n",
      "Done waiting... Attempting to copy over binary\n",
      "Copied parameter server\n",
      "Defining configuration file\n",
      "\n",
      "input_path: /home/ec2-user/cirrus/examples/ml/tests/test_mf/nf_parsed \n",
      "input_type: csv \n",
      "minibatch_size: 20 \n",
      "s3_size: 10000 \n",
      "model_type: CollaborativeFiltering \n",
      "learning_rate: 0.010000 \n",
      "epsilon: 0.000100 \n",
      "normalize: 1 \n",
      "limit_samples: 50000000 \n",
      "use_bias: 1 \n",
      "num_users: 480189 \n",
      "num_items: 17770 \n",
      "train_set: 0-5 \n",
      "s3_bucket: cirrus-netflix-not-normalized\n",
      "\n",
      "Launching ps\n",
      "Launching parameter server\n",
      "('cmd:', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-54-188-0-164.us-west-2.compute.amazonaws.com \"nohup ./parameter_server --config config_cf.txt --nworkers 10000 --rank 1 &> ps_output &\"')\n",
      "Launching lambdas\n",
      "Starting error taskLambdas have been launched\n",
      "User's specific ip: ec2-54-188-0-164.us-west-2.compute.amazonaws.com\n",
      "\n",
      "('cmd', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-54-188-0-164.us-west-2.compute.amazonaws.com \"./parameter_server --config config_cf.txt --nworkers 10 --rank 2 --ps_ip \"172.31.26.54\"\" > error_out &')\n",
      "Copying ps to vm\n",
      "Cost Model\n",
      "Done waiting... Attempting to copy over binary\n",
      "Copied parameter server\n",
      "Defining configuration file\n",
      "\n",
      "input_path: /home/ec2-user/cirrus/examples/ml/tests/test_mf/nf_parsed \n",
      "input_type: csv \n",
      "minibatch_size: 20 \n",
      "s3_size: 10000 \n",
      "model_type: CollaborativeFiltering \n",
      "learning_rate: 0.010000 \n",
      "epsilon: 0.000100 \n",
      "normalize: 1 \n",
      "limit_samples: 50000000 \n",
      "use_bias: 1 \n",
      "num_users: 480189 \n",
      "num_items: 17770 \n",
      "train_set: 0-5 \n",
      "s3_bucket: cirrus-netflix-not-normalized\n",
      "\n",
      "Launching ps\n",
      "Launching parameter server\n",
      "('cmd:', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-54-188-0-164.us-west-2.compute.amazonaws.com \"nohup ./parameter_server --config config_cf.txt --nworkers 10000 --rank 1 &> ps_output &\"')\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.26.54\"}\n",
      "Launching lambdas\n",
      "Starting error task\n",
      "('cmd', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-54-188-0-164.us-west-2.compute.amazonaws.com \"./parameter_server --config config_cf.txt --nworkers 10 --rank 2 --ps_ip \"172.31.26.54\"\" > error_out &')\n",
      "Lambdas have been launched\n",
      "Cost Model\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.26.54\"}\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.26.54\"}\n",
      "Current training loss: (1.0445, 1.08849) current cost ($):  0.000510444996389\n",
      "Current training loss: (1.0445, 1.08849) current cost ($):  0.000255098854764\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.26.54\"}\n",
      "PS has 7 lambdas\n",
      "Current training loss: (2.0817, 1.08849) current cost ($):  0.000638087426058\n",
      "Current training loss: (3.1018, 1.08528) current cost ($):  0.000638276089795\n",
      "Current training loss: (2.0817, 1.08849) current cost ($):  0.000382931604449\n",
      "Current training loss: (3.1018, 1.08528) current cost ($):  0.000383219416936\n",
      "Current training loss: (4.1331, 1.07329) current cost ($):  0.000766024384499\n",
      "Current training loss: (5.1727, 1.05774) current cost ($):  0.000766351978048\n",
      "Current training loss: (4.1331, 1.07329) current cost ($):  0.000511166069921\n",
      "Current training loss: (5.1727, 1.05774) current cost ($):  0.000511326349004\n",
      "PS has 7 lambdas\n",
      "Current training loss: (6.1945, 1.05075) current cost ($):  0.000894169502131\n",
      "Current training loss: (7.2296, 1.04261) current cost ($):  0.000894487644259\n",
      "Current training loss: (6.1945, 1.05075) current cost ($):  0.000639001174863\n",
      "Current training loss: (7.2296, 1.04261) current cost ($):  0.000639195688756\n",
      "PS has 7 lambdas\n",
      "Current training loss: (8.2714, 1.03793) current cost ($):  0.00102226845671\n",
      "Current training loss: (9.299, 1.03456) current cost ($):  0.00102248637123\n",
      "Current training loss: (8.2714, 1.03793) current cost ($):  0.000766909125646\n",
      "Current training loss: (9.299, 1.03456) current cost ($):  0.000767170878347\n",
      "PS has 7 lambdas\n",
      "Current training loss: (10.315, 1.03318) current cost ($):  0.00115049170812\n",
      "Current training loss: (11.353, 1.03143) current cost ($):  0.00115075198689\n",
      "Current training loss: (10.315, 1.03318) current cost ($):  0.000895019947688\n",
      "Current training loss: (11.353, 1.03143) current cost ($):  0.000895269270706\n",
      "PS has 7 lambdas\n",
      "Current training loss: (12.39, 1.03001) current cost ($):  0.00127861717834\n",
      "Current training loss: (13.423, 1.02968) current cost ($):  0.00127896937294\n",
      "PS has 7 lambdas\n",
      "Current training loss: (12.39, 1.03001) current cost ($):  0.00102314459737\n",
      "Current training loss: (13.423, 1.02968) current cost ($):  0.00102339366207\n",
      "PS has 7 lambdas\n",
      "Current training loss: (14.478, 1.02934) current cost ($):  0.00140673953355\n",
      "Current training loss: (15.532, 1.02786) current cost ($):  0.00140690894496\n",
      "Current training loss: (14.478, 1.02934) current cost ($):  0.0011512491134\n",
      "Current training loss: (15.532, 1.02786) current cost ($):  0.00115132004464\n",
      "PS has 7 lambdas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "try:\n",
    "    lr_task.run()\n",
    "    lr_task1.run()\n",
    "except KeyboardInterrupt:\n",
    "    lr_task.kill()\n",
    "    lr_task1.kill()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock received kill command\n",
      "Mock is dead\n"
     ]
    }
   ],
   "source": [
    "mock_obj.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
