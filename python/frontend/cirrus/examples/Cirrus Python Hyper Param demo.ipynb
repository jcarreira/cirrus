{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cirrus Demo\n",
    "\n",
    "## Simple Example\n",
    "\n",
    "This will run a simple logistic regression on the Criteo Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.12 (default, Dec  4 2017, 14:50:18) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.genUID = function() {\n",
       "    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {\n",
       "        var r = Math.random()*16|0, v = c == 'x' ? r : (r&0x3|0x8);\n",
       "        return v.toString(16);\n",
       "    });\n",
       "};\n",
       "\n",
       "\n",
       "define('graphWidget', [\"@jupyter-widgets/base\"], function (widget) {\n",
       "\n",
       "    var GraphView = widget.DOMWidgetView.extend({\n",
       "        render: function(){\n",
       "            var that = this;\n",
       "\n",
       "            var graphId = window.genUID();\n",
       "            var loadingId = 'loading-'+graphId;\n",
       "\n",
       "\n",
       "            var _graph_url = that.model.get('_graph_url');\n",
       "\n",
       "            // variable plotlyDomain in the case of enterprise\n",
       "            var url_parts = _graph_url.split('/');\n",
       "            var plotlyDomain = url_parts[0] + '//' + url_parts[2];\n",
       "\n",
       "            if(!('plotlyDomains' in window)){\n",
       "                window.plotlyDomains = {};\n",
       "            }\n",
       "            window.plotlyDomains[graphId] = plotlyDomain;\n",
       "\n",
       "            // Place IFrame in output cell div `$el`\n",
       "            that.$el.css('width', '100%');\n",
       "            that.$graph = $(['<iframe id=\"'+graphId+'\"',\n",
       "                             'src=\"'+_graph_url+'.embed\"',\n",
       "                             'seamless',\n",
       "                             'style=\"border: none;\"',\n",
       "                             'width=\"100%\"',\n",
       "                             'height=\"600\">',\n",
       "                             '</iframe>'].join(' '));\n",
       "            that.$graph.appendTo(that.$el);\n",
       "\n",
       "            that.$loading = $('<div id=\"'+loadingId+'\">Initializing...</div>')\n",
       "                            .appendTo(that.$el);\n",
       "\n",
       "            // for some reason the 'width' is being changed in IPython 3.0.0\n",
       "            // for the containing `div` element. There's a flicker here, but\n",
       "            // I was unable to fix it otherwise.\n",
       "            setTimeout(function ()  {\n",
       "                if (IPYTHON_VERSION === '3') {\n",
       "                    $('#' + graphId)[0].parentElement.style.width = '100%';\n",
       "                }\n",
       "            }, 500);\n",
       "\n",
       "            // initialize communication with the iframe\n",
       "            if(!('pingers' in window)){\n",
       "                window.pingers = {};\n",
       "            }\n",
       "\n",
       "            window.pingers[graphId] = setInterval(function() {\n",
       "                that.graphContentWindow = $('#'+graphId)[0].contentWindow;\n",
       "                that.graphContentWindow.postMessage({task: 'ping'}, plotlyDomain);\n",
       "            }, 200);\n",
       "\n",
       "            // Assign a message listener to the 'message' events\n",
       "            // from iframe's postMessage protocol.\n",
       "            // Filter the messages by iframe src so that the right message\n",
       "            // gets passed to the right widget\n",
       "            if(!('messageListeners' in window)){\n",
       "                 window.messageListeners = {};\n",
       "            }\n",
       "\n",
       "            window.messageListeners[graphId] = function(e) {\n",
       "                if(_graph_url.indexOf(e.origin)>-1) {\n",
       "                    var frame = document.getElementById(graphId);\n",
       "\n",
       "                    if(frame === null){\n",
       "                        // frame doesn't exist in the dom anymore, clean up it's old event listener\n",
       "                        window.removeEventListener('message', window.messageListeners[graphId]);\n",
       "                        clearInterval(window.pingers[graphId]);\n",
       "                    } else if(frame.contentWindow === e.source) {\n",
       "                        // TODO: Stop event propagation, so each frame doesn't listen and filter\n",
       "                        var frameContentWindow = $('#'+graphId)[0].contentWindow;\n",
       "                        var message = e.data;\n",
       "\n",
       "                        if('pong' in message && message.pong) {\n",
       "                            $('#loading-'+graphId).hide();\n",
       "                            clearInterval(window.pingers[graphId]);\n",
       "                            that.send({event: 'pong', graphId: graphId});\n",
       "                        } else if (message.type==='hover' ||\n",
       "                                   message.type==='zoom'  ||\n",
       "                                   message.type==='click' ||\n",
       "                                   message.type==='unhover') {\n",
       "\n",
       "                            // click and hover events contain all of the data in the traces,\n",
       "                            // which can be a very large object and may take a ton of time\n",
       "                            // to pass to the python backend. Strip out the data, and require\n",
       "                            // the user to call get_figure if they need trace information\n",
       "                            if(message.type !== 'zoom') {\n",
       "                                for(var i in message.points) {\n",
       "                                    delete message.points[i].data;\n",
       "                                    delete message.points[i].fullData;\n",
       "                                }\n",
       "                            }\n",
       "                            that.send({event: message.type, message: message, graphId: graphId});\n",
       "                        } else if (message.task === 'getAttributes') {\n",
       "                            that.send({event: 'getAttributes', response: message.response});\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            };\n",
       "\n",
       "            window.removeEventListener('message', window.messageListeners[graphId]);\n",
       "            window.addEventListener('message', window.messageListeners[graphId]);\n",
       "\n",
       "        },\n",
       "\n",
       "        update: function() {\n",
       "            // Listen for messages from the graph widget in python\n",
       "            var jmessage = this.model.get('_message');\n",
       "            var message = JSON.parse(jmessage);\n",
       "\n",
       "            // check for duplicate messages\n",
       "            if(!('messageIds' in window)){\n",
       "                window.messageIds = {};\n",
       "            }\n",
       "\n",
       "            if(!(message.uid in window.messageIds)){\n",
       "                // message hasn't been received yet, do stuff\n",
       "                window.messageIds[message.uid] = true;\n",
       "\n",
       "                if (message.fadeTo) {\n",
       "                    this.fadeTo(message);\n",
       "                } else {\n",
       "                    var plot = $('#' + message.graphId)[0].contentWindow;\n",
       "                    plot.postMessage(message, window.plotlyDomains[message.graphId]);\n",
       "                }\n",
       "            }\n",
       "\n",
       "            return GraphView.__super__.update.apply(this);\n",
       "        },\n",
       "\n",
       "        /**\n",
       "         * Wrapper for jquery's `fadeTo` function.\n",
       "         *\n",
       "         * @param message Contains the id we need to find the element.\n",
       "         */\n",
       "        fadeTo: function (message) {\n",
       "            var plot = $('#' + message.graphId);\n",
       "            plot.fadeTo(message.duration, message.opacity);\n",
       "        }\n",
       "    });\n",
       "\n",
       "    // Register the GraphView with the widget manager.\n",
       "    return {\n",
       "        GraphView: GraphView\n",
       "    }\n",
       "\n",
       "});\n",
       "\n",
       "//@ sourceURL=graphWidget.js\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cirrus\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import tools\n",
    "import plotly.tools as tls   \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "from __future__ import print_function\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display, clear_output\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.widgets import GraphWidget\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import random\n",
    "\n",
    "class mock():\n",
    "    \n",
    "    def __init__(self, strid):\n",
    "        self.pipe = py.Stream(strid)\n",
    "        self.pipe.open()\n",
    "        self.kill_sig = threading.Event()\n",
    "    \n",
    "    def start_thread(self):\n",
    "        \n",
    "        def num_producer():\n",
    "            start_time = time.time()\n",
    "            \n",
    "            while not self.kill_sig.is_set():\n",
    "                time.sleep(0.5)\n",
    "                now_time = time.time()\n",
    "                integer = random.random()\n",
    "                self.pipe.write(dict(x = now_time - start_time, y = integer))\n",
    "        \n",
    "        self.thr = threading.Thread(target=num_producer)\n",
    "        self.thr.start()\n",
    "        \n",
    "    def kill(self):\n",
    "        print(\"Mock received kill command\")\n",
    "        self.kill_sig.set()\n",
    "        self.thr.join()\n",
    "        self.pipe.close()\n",
    "        print(\"Mock is dead\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Logistic Regression workload\n",
      "Starting LogisticRegressionTask\n",
      "Running Logistic Regression workload\n",
      "Starting LogisticRegressionTask\n"
     ]
    }
   ],
   "source": [
    "import cirrus\n",
    "\n",
    "\n",
    "\n",
    "data_bucket = 'cirrus-criteo-kaggle-19b-random'\n",
    "model = 'model_v1'\n",
    "\n",
    "lr_task = cirrus.LogisticRegression(\n",
    "             # number of workers\n",
    "             n_workers = 5,\n",
    "             # number of parameter servers\n",
    "             n_ps = 2,\n",
    "             # worker size in MB\n",
    "             worker_size = 128,\n",
    "             # path to s3 bucket with input dataset\n",
    "             dataset = data_bucket,\n",
    "             # sgd update LR and epsilon\n",
    "             learning_rate=0.01,\n",
    "             epsilon=0.0001,\n",
    "             progress_callback = print,\n",
    "             # stop workload after these many seconds\n",
    "             timeout = 0,\n",
    "             # stop workload once we reach this loss\n",
    "             threshold_loss=0,\n",
    "             # resume execution from model stored in this s3 bucket\n",
    "             resume_model = model,\n",
    "             # aws key name\n",
    "             key_name='mykey',\n",
    "             # path to aws key\n",
    "             key_path='/home/camus/Downloads/mykey.pem',\n",
    "             # ip where ps lives\n",
    "             ps_ip_public='ec2-18-237-250-161.us-west-2.compute.amazonaws.com',\n",
    "             ps_ip_private='172.31.10.1',\n",
    "             # username of VM\n",
    "             ps_username='ubuntu',\n",
    "             # choose between adagrad, sgd, nesterov, momentum\n",
    "             opt_method = 'adagrad',\n",
    "             # checkpoint model every x secs\n",
    "             checkpoint_model = 60,\n",
    "             #\n",
    "             minibatch_size=20,\n",
    "             # model size\n",
    "             model_bits=19,\n",
    "             # whether to filter gradient weights\n",
    "             use_grad_threshold=False,\n",
    "             # threshold value\n",
    "             grad_threshold=0.001,\n",
    "             # range of training minibatches\n",
    "             train_set=(0,824),\n",
    "             # range of testing minibatches\n",
    "             test_set=(835,840)\n",
    "             )\n",
    "\n",
    "lr_task1 = cirrus.LogisticRegression(\n",
    "             # number of workers\n",
    "             n_workers = 5,\n",
    "             # number of parameter servers\n",
    "             n_ps = 2,\n",
    "             # worker size in MB\n",
    "             worker_size = 128,\n",
    "             # path to s3 bucket with input dataset\n",
    "             dataset = data_bucket,\n",
    "             # sgd update LR and epsilon\n",
    "             learning_rate=0.01,\n",
    "             epsilon=0.0001,\n",
    "             progress_callback = print,\n",
    "             # stop workload after these many seconds\n",
    "             timeout = 0,\n",
    "             # stop workload once we reach this loss\n",
    "             threshold_loss=0,\n",
    "             # resume execution from model stored in this s3 bucket\n",
    "             resume_model = model,\n",
    "             # aws key name\n",
    "             key_name='mykey',\n",
    "             # path to aws key\n",
    "             key_path='/home/camus/Downloads/mykey.pem',\n",
    "             # ip where ps lives\n",
    "             ps_ip_public='ec2-34-216-121-188.us-west-2.compute.amazonaws.com',\n",
    "             ps_ip_private='172.31.0.131',\n",
    "             # username of VM\n",
    "             ps_username='ubuntu',\n",
    "             # choose between adagrad, sgd, nesterov, momentum\n",
    "             opt_method = 'adagrad',\n",
    "             # checkpoint model every x secs\n",
    "             checkpoint_model = 60,\n",
    "             #\n",
    "             minibatch_size=20,\n",
    "             # model size\n",
    "             model_bits=19,\n",
    "             # whether to filter gradient weights\n",
    "             use_grad_threshold=False,\n",
    "             # threshold value\n",
    "             grad_threshold=0.001,\n",
    "             # range of training minibatches\n",
    "             train_set=(0,824),\n",
    "             # range of testing minibatches\n",
    "             test_set=(835,840)\n",
    "             )\n",
    "\n",
    "\n",
    "#model, loss = lr_task.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://plot.ly/~andrewmzhang/16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb166e09f0f4d63b07166d4250a3721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'y': 0.499617, 'x': 27.23, 'pointNumber': 9, 'curveNumber': 1}]\n",
      "Lambda launcher has received kill signal\n",
      "Error task has received kill signal\n",
      "Everyone is dead\n",
      "Current training loss: (36.366, 0.498158) current cost ($):  0.00264741428579\n",
      "PS has 49 lambdas\n",
      "Current training loss: (38.7, 0.497934) current cost ($):  0.00277526838474\n",
      "PS has 47 lambdas\n",
      "Current training loss: (41.022, 0.497845) current cost ($):  0.00290287360134\n",
      "Current training loss: (43.389, 0.497809) current cost ($):  0.00303072177416\n",
      "PS has 47 lambdas\n",
      "Current training loss: (45.649, 0.497763) current cost ($):  0.00315835422058\n",
      "PS has 49 lambdas\n",
      "Current training loss: (47.782, 0.497687) current cost ($):  0.00328629020653\n",
      "PS has 45 lambdas\n",
      "Current training loss: (50.031, 0.497564) current cost ($):  0.00354175781867\n",
      "PS has 43 lambdas\n",
      "Current training loss: (52.14, 0.497455) current cost ($):  0.0036695221899\n",
      "Current training loss: (54.272, 0.497383) current cost ($):  0.00379741058445\n",
      "PS has 41 lambdas\n",
      "Current training loss: (56.498, 0.49738) current cost ($):  0.0039250187945\n",
      "PS has 41 lambdas\n",
      "Current training loss: (58.687, 0.497478) current cost ($):  0.00405271325258\n",
      "Current training loss: (60.84, 0.497584) current cost ($):  0.00418066511752\n",
      "PS has 39 lambdas\n",
      "Current training loss: (62.97, 0.497699) current cost ($):  0.00430869200122\n",
      "Current training loss: (65.31, 0.497755) current cost ($):  0.00443636140238\n",
      "PS has 33 lambdas\n",
      "Current training loss: (67.465, 0.497792) current cost ($):  0.0045641534289\n",
      "PS has 34 lambdas\n",
      "Current training loss: (69.691, 0.49784) current cost ($):  0.00469194493879\n",
      "Current training loss: (71.847, 0.497963) current cost ($):  0.00481962064597\n",
      "PS has 34 lambdas\n",
      "PS has 36 lambdas\n",
      "Current training loss: (74.595, 0.498091) current cost ($):  0.00507476060378\n",
      "PS has 32 lambdas\n",
      "Current training loss: (76.717, 0.498197) current cost ($):  0.00520235015411\n",
      "Current training loss: (79.408, 0.498149) current cost ($):  0.00532998667285\n",
      "PS has 32 lambdas\n",
      "Current training loss: (82.134, 0.497997) current cost ($):  0.00558513020153\n",
      "PS has 28 lambdas\n",
      "Current training loss: (84.447, 0.497747) current cost ($):  0.00571278749212\n",
      "PS has 26 lambdas\n",
      "Current training loss: (86.563, 0.497634) current cost ($):  0.00584043554401\n",
      "Current training loss: (89.178, 0.497525) current cost ($):  0.0059682115092\n",
      "PS has 22 lambdas\n",
      "Current training loss: (91.336, 0.497381) current cost ($):  0.00609614489676\n",
      "PS has 26 lambdas\n",
      "PS has 34 lambdas\n",
      "Current training loss: (93.933, 0.497395) current cost ($):  0.00635134201895\n",
      "Current training loss: (96.365, 0.497441) current cost ($):  0.00647925589587\n",
      "PS has 28 lambdas\n",
      "Current training loss: (99.006, 0.49753) current cost ($):  0.00660701248716\n",
      "Current training loss: (101.11, 0.497568) current cost ($):  0.00673490992285\n",
      "PS has 32 lambdas\n",
      "Current training loss: (103.17, 0.497608) current cost ($):  0.00686276174253\n",
      "PS has 34 lambdas\n",
      "Current training loss: (105.86, 0.497697) current cost ($):  0.00699034159832\n",
      "PS has 34 lambdas\n",
      "Current training loss: (107.98, 0.497784) current cost ($):  0.00724553024158\n",
      "PS has 34 lambdas\n",
      "Current training loss: (110.1, 0.497884) current cost ($):  0.00737329034297\n",
      "Current training loss: (112.33, 0.497906) current cost ($):  0.00750109511827\n",
      "PS has 38 lambdas\n",
      "Current training loss: (114.88, 0.497875) current cost ($):  0.00762909866212\n",
      "PS has 34 lambdas\n",
      "Current training loss: (116.95, 0.497865) current cost ($):  0.00775700317879\n",
      "Current training loss: (119.11, 0.497859) current cost ($):  0.00788491068891\n",
      "PS has 34 lambdas\n",
      "Current training loss: (121.0, 0.497846) current cost ($):  0.0080125793911\n",
      "PS has 28 lambdas\n",
      "Current training loss: (124.09, 0.497847) current cost ($):  0.00826806675701\n",
      "PS has 22 lambdas\n",
      "Current training loss: (126.18, 0.497806) current cost ($):  0.00839590594947\n",
      "Current training loss: (128.86, 0.497819) current cost ($):  0.00852357993959\n",
      "PS has 20 lambdas\n",
      "Current training loss: (130.97, 0.497954) current cost ($):  0.00865149904366\n",
      "PS has 18 lambdas\n",
      "Current training loss: (133.54, 0.497976) current cost ($):  0.0087793179657\n",
      "Current training loss: (135.65, 0.497965) current cost ($):  0.00890702822679\n",
      "PS has 16 lambdas\n",
      "PS has 16 lambdas\n",
      "Current training loss: (138.4, 0.497975) current cost ($):  0.00916248716246\n",
      "Current training loss: (140.91, 0.497957) current cost ($):  0.00929032916603\n",
      "PS has 16 lambdas\n",
      "Current training loss: (142.98, 0.497857) current cost ($):  0.00941803246771\n",
      "PS has 16 lambdas\n",
      "Current training loss: (145.11, 0.497749) current cost ($):  0.00954593558642\n",
      "Current training loss: (147.79, 0.497623) current cost ($):  0.00967382524217\n",
      "PS has 16 lambdas\n",
      "PS has 18 lambdas\n",
      "Current training loss: (150.45, 0.497561) current cost ($):  0.0099290098739\n"
     ]
    }
   ],
   "source": [
    "stream_ids = tls.get_credentials_file()['stream_ids']\n",
    "\n",
    "# Get stream id from stream id list \n",
    "stream_id0 = stream_ids[0]\n",
    "stream_id1 = stream_ids[1]\n",
    "stream_id2 = stream_ids[2]\n",
    "stream_id3 = stream_ids[3]\n",
    "\n",
    "\n",
    "stream_0 = go.Stream(\n",
    "    token=stream_id0,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_1 = go.Stream(\n",
    "    token=stream_id1,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_2 = go.Stream(\n",
    "    token=stream_id2,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "stream_3 = go.Stream(\n",
    "    token=stream_id3,\n",
    "    maxpoints=80\n",
    ")\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    mode='lines+markers',\n",
    "    stream=stream_0         # (!) embed stream id, 1 per trace\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x2',\n",
    "    yaxis='y2',\n",
    "    stream=stream_1\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x3',\n",
    "    yaxis='y3',\n",
    "    stream=stream_2\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    xaxis='x4',\n",
    "    yaxis='y4',\n",
    "    stream=stream_3\n",
    ")\n",
    "data = [trace0, trace1, trace2, trace3]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        domain=[0, 0.45]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        domain=[0, 0.45]\n",
    "    ),\n",
    "    xaxis2=dict(\n",
    "        domain=[0.55, 1]\n",
    "    ),\n",
    "    xaxis3=dict(\n",
    "        domain=[0, 0.45],\n",
    "        anchor='y3'\n",
    "    ),\n",
    "    xaxis4=dict(\n",
    "        domain=[0.55, 1],\n",
    "        anchor='y4'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        domain=[0, 0.45],\n",
    "        anchor='x2'\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        domain=[0.55, 1]\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        domain=[0.55, 1],\n",
    "        anchor='x4'\n",
    "    )\n",
    ")\n",
    "\n",
    "    \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "url = py.plot(fig, filename='multiple-subplots', auto_open=False)\n",
    "print(url)\n",
    "g = GraphWidget(url)\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_handler(widget, msg):\n",
    "    print(msg)\n",
    "    global lr_task\n",
    "    if msg[0]['curveNumber'] == 0:\n",
    "        lr_task.kill()\n",
    "    if msg[0]['curveNumber'] == 1:\n",
    "        lr_task1.kill()\n",
    "\n",
    "pipe0 = py.Stream(stream_id0)\n",
    "pipe0.open()\n",
    "\n",
    "pipe1 = py.Stream(stream_id1)\n",
    "pipe1.open()\n",
    "\n",
    "def progress_callback0(time_loss, cost, task):\n",
    "    global pipe0\n",
    "    print(\"Current training loss:\", time_loss, \"current cost ($): \", cost)\n",
    "    pipe0.write(dict(x=time_loss[0], y=time_loss[1]))\n",
    "    \n",
    "def progress_callback1(time_loss, cost, task):\n",
    "    global pipe1\n",
    "    print(\"Current training loss:\", time_loss, \"current cost ($): \", cost)\n",
    "    pipe1.write(dict(x=time_loss[0], y=time_loss[1]))\n",
    "\n",
    "\n",
    "lr_task.progress_callback = progress_callback0\n",
    "lr_task1.progress_callback = progress_callback1\n",
    "\n",
    "\n",
    "g.on_click(message_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d897b1c3ee24fc2b4df4d7a29c210fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description=u'Halt!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(description=\"Halt!\")\n",
    "display(button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    lr_task.kill()\n",
    "    lr_task1.kill()\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's specific ip: ec2-18-237-250-161.us-west-2.compute.amazonaws.com\n",
      "Copying ps to vm..\n",
      "Done waiting... Attempting to copy over binary\n",
      "Copied PS binary to VM\n",
      "Defining configuration file\n",
      "\n",
      "input_path: /mnt/efs/criteo_kaggle/train.csv \n",
      "input_type: csv\n",
      "num_classes: 2 \n",
      "num_features: 13 \n",
      "limit_cols: 14 \n",
      "normalize: 1 \n",
      "limit_samples: 50000000 \n",
      "s3_size: 50000 \n",
      "use_bias: 1 \n",
      "model_type: LogisticRegression \n",
      "minibatch_size: 20 \n",
      "learning_rate: 0.010000 \n",
      "epsilon: 0.000100 \n",
      "model_bits: 19 \n",
      "s3_bucket: cirrus-criteo-kaggle-19b-random \n",
      "use_grad_threshold: 0 \n",
      "grad_threshold: 0.001000 \n",
      "train_set: 0-824 \n",
      "test_set: 835-840\n",
      "\n",
      "Launching ps\n",
      "Launching parameter server\n",
      "('cmd:', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-18-237-250-161.us-west-2.compute.amazonaws.com \"nohup ./parameter_server --config config_lr.txt --nworkers 10000 --rank 1 &> ps_output &\"')\n",
      "Launching lambdas\n",
      "Starting error taskLambdas have been launched\n",
      "User's specific ip: ec2-34-216-121-188.us-west-2.compute.amazonaws.com\n",
      "('cmd', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-18-237-250-161.us-west-2.compute.amazonaws.com \"./parameter_server --config config_lr.txt --nworkers 10 --rank 2 --ps_ip \"172.31.10.1\"\" > error_out &')\n",
      "\n",
      "Copying ps to vm..\n",
      "Cost Model\n",
      "Done waiting... Attempting to copy over binary\n",
      "Copied PS binary to VM\n",
      "Defining configuration file\n",
      "\n",
      "input_path: /mnt/efs/criteo_kaggle/train.csv \n",
      "input_type: csv\n",
      "num_classes: 2 \n",
      "num_features: 13 \n",
      "limit_cols: 14 \n",
      "normalize: 1 \n",
      "limit_samples: 50000000 \n",
      "s3_size: 50000 \n",
      "use_bias: 1 \n",
      "model_type: LogisticRegression \n",
      "minibatch_size: 20 \n",
      "learning_rate: 0.010000 \n",
      "epsilon: 0.000100 \n",
      "model_bits: 19 \n",
      "s3_bucket: cirrus-criteo-kaggle-19b-random \n",
      "use_grad_threshold: 0 \n",
      "grad_threshold: 0.001000 \n",
      "train_set: 0-824 \n",
      "test_set: 835-840\n",
      "\n",
      "Launching ps\n",
      "Launching parameter server\n",
      "('cmd:', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-34-216-121-188.us-west-2.compute.amazonaws.com \"nohup ./parameter_server --config config_lr.txt --nworkers 10000 --rank 1 &> ps_output &\"')\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.10.1\"}\n",
      "Launching lambdas\n",
      "Starting error task\n",
      " ('cmd', 'ssh -o \"StrictHostKeyChecking no\" -i /home/camus/Downloads/mykey.pem ubuntu@ec2-34-216-121-188.us-west-2.compute.amazonaws.com \"./parameter_server --config config_lr.txt --nworkers 10 --rank 2 --ps_ip \"172.31.0.131\"\" > error_out &')Lambdas have been launched\n",
      "\n",
      "Cost Model\n",
      "PS has 7 lambdas\n",
      "Current training loss: (4.4931, 0.525525) current cost ($):  0.000127584505526\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (4.4931, 0.525525) current cost ($):  0.000510256970851\n",
      "PS has 11 lambdas\n",
      "Current training loss: (7.2273, 0.516991) current cost ($):  0.0006382180137\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (7.2273, 0.516991) current cost ($):  0.00038276226902\n",
      "PS has 15 lambdas\n",
      "Current training loss: (9.3978, 0.510875) current cost ($):  0.000766063633728\n",
      "Current training loss: (9.3978, 0.510875) current cost ($):  0.000510515410995\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "PS has 19 lambdas\n",
      "Current training loss: (11.885, 0.507758) current cost ($):  0.000638299338468\n",
      "Current training loss: (11.885, 0.507758) current cost ($):  0.00102141314869\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "PS has 19 lambdas\n",
      "Current training loss: (14.78, 0.505689) current cost ($):  0.00114923200995\n",
      "Current training loss: (14.78, 0.505689) current cost ($):  0.000893716411336\n",
      "Current training loss: (17.54, 0.503355) current cost ($):  0.0013694283165\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (17.54, 0.503355) current cost ($):  0.00111411080265\n",
      "PS has 23 lambdas\n",
      "Current training loss: (19.685, 0.501544) current cost ($):  0.00149715641677\n",
      "Current training loss: (19.685, 0.501544) current cost ($):  0.00124178345559\n",
      "PS has 27 lambdas\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (22.489, 0.50074) current cost ($):  0.00136951289304\n",
      "Current training loss: (22.489, 0.50074) current cost ($):  0.00175249344126\n",
      "PS has 33 lambdas\n",
      "Current training loss: (24.593, 0.500061) current cost ($):  0.00149715284589\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (24.593, 0.500061) current cost ($):  0.00188039967499\n",
      "PS has 41 lambdas\n",
      "Current training loss: (27.23, 0.499617) current cost ($):  0.00200833835049\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (27.23, 0.499617) current cost ($):  0.00175255940367\n",
      "PS has 45 lambdas\n",
      "Current training loss: (29.863, 0.499235) current cost ($):  0.00188046545506\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (29.863, 0.499235) current cost ($):  0.00226384866117\n",
      "Current training loss: (32.028, 0.49878) current cost ($):  0.00200838673204\n",
      "PS has 45 lambdas\n",
      "Current training loss: (32.028, 0.49878) current cost ($):  0.00239166407312\n",
      "PS has 0 lambdas\n",
      "payload: {\"num_task\": 3, \"num_workers\": 5, \"ps_ip\": \"172.31.0.131\"}\n",
      "Current training loss: (34.11, 0.498466) current cost ($):  0.00213619442482\n",
      "Current training loss: (34.11, 0.498466) current cost ($):  0.00251961683458\n",
      "PS has 49 lambdas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "try:\n",
    "    lr_task.run()\n",
    "    lr_task1.run()\n",
    "except KeyboardInterrupt:\n",
    "    lr_task.kill()\n",
    "    lr_task1.kill()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_obj.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
